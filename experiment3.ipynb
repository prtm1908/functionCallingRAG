{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "os.environ[\"LLAMA_CLOUD_API_KEY\"] = \"llx-SMDhxGbX6ZiXu2EAvqikEdggBh5rPDoTbUwVpE2W79xMAk21\"\n",
    "\n",
    "os.environ[\"HF_TOKEN\"]=\"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\"\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyAuhDMHPn0oNbKGlGUVzA5N0aAJkdkhM-E\"\n",
    "genai.configure(api_key=\"AIzaSyAuhDMHPn0oNbKGlGUVzA5N0aAJkdkhM-E\")\n",
    "model = genai.GenerativeModel('gemini-pro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id b1447971-0b03-4f52-95ec-c2d542310a5c\n"
     ]
    }
   ],
   "source": [
    "from llama_parse import LlamaParse\n",
    "\n",
    "parser = LlamaParse(verbose=True)\n",
    "json_objs = parser.get_json_result(\"GemmaPDF.pdf\")\n",
    "json_list = json_objs[0][\"pages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import TextNode\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def get_text_nodes(json_list: List[dict]):\n",
    "    text_nodes = []\n",
    "    for idx, page in enumerate(json_list):\n",
    "        text_node = TextNode(\n",
    "            text=page[\"text\"],\n",
    "            metadata={\n",
    "                \"page\": page[\"page\"]\n",
    "            }\n",
    "        )\n",
    "        text_nodes.append(text_node)\n",
    "    return text_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_nodes = get_text_nodes(json_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex(text_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_retriever = index.as_retriever(similarity_top_k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, inspect, json, re\n",
    "import xml.etree.ElementTree as ET\n",
    "from functools import partial\n",
    "from typing import get_type_hints\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "from langchain.chains.openai_functions import convert_to_openai_function\n",
    "from langchain.utils.openai_functions import convert_pydantic_to_openai_function\n",
    "from langchain.pydantic_v1 import BaseModel, Field, validator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchPaperAnswer(BaseModel):\n",
    "    \"\"\"Provides answers of questions based on given research paper extract\"\"\"\n",
    "    question: str = Field(description=\"question of user from an extract of  a research paper\")\n",
    "    extract: str = Field(description=\"extract of research paper whose information has to be used to answer the question\")\n",
    "    answer: str = Field(description=\"answer of the question generated by you based on the extract\")\n",
    "\n",
    "    @validator(\"question\")\n",
    "    def question_must_not_be_empty(cls, field):\n",
    "        if not field:\n",
    "            raise ValueError(\"Question cannot be empty.\")\n",
    "        return field\n",
    "    @validator(\"extract\")\n",
    "    def extract_must_not_be_empty(cls, field):\n",
    "        if not field:\n",
    "            raise ValueError(\"Extract cannot be empty.\")\n",
    "        return field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_print_class_name(json_string):\n",
    "    try:\n",
    "        data = json.loads(json_string)\n",
    "        if \"name\" in data and \"arguments\" in data and all(key in data[\"arguments\"] for key in [\"question\", \"extract\", \"answer\"]):\n",
    "            print(\"Detected class: \", data[\"name\"])\n",
    "        else:\n",
    "            print(\"Detected JSON but not matching the specified format\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Not a valid JSON string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = \"\"\"{\"name\": \"function_name\", \"arguments\": {\"arg_1\": \"value_1\", \"arg_2\": value_2, ...}}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makePrompt(context):\n",
    "    prompt = f\"\"\"You are a helpful assistant with access to the following functions:\n",
    "\n",
    "    {convert_pydantic_to_openai_function(ResearchPaperAnswer)}\n",
    "\n",
    "    To use these functions respond with:\n",
    "\n",
    "        {fn} \n",
    "        {fn} \n",
    "        ...\n",
    "\n",
    "    Answer the following question: {query}\n",
    "\n",
    "    You have a context below that you must use and consider while answering the question, do not make up any information by yourself that is not present in it.\n",
    "\n",
    "    Context: {context}\n",
    "\n",
    "\n",
    "    Edge cases you must handle:\n",
    "    - If there are no functions that match the user request, you will respond politely that you cannot help.\"\"\"\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"Tell about 2B and 7B model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrievals = base_retriever.retrieve(query)\n",
    "context=retrievals[0].node.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"name\": \"ResearchPaperAnswer\", \"arguments\": {\"question\": \"Tell about 2B and 7B model\", \"extract\": \"In this technical report, we provide a detailed overview of the model architecture, training infrastructure, and pretraining and fine-tuning recipes for Gemma, followed by thorough evaluations of all checkpoints across a wide-variety of quantitative and qualitative benchmarks, as well as both standard academic benchmarks and human-preference evaluations. We then discuss in detail our approach to safe and responsible deployment. Finally, we outline the broader implications of Gemma, its limitations and advantages.\\n\\nModel Architecture\\nThe Gemma model architecture is based on the transformer decoder (Vaswani et al., 2017). The core parameters of the architecture are summa-rized in Table 1. Models are trained on a context length of 8192 tokens. We also utilize several improvements proposed after the original trans-\\nformer paper, and list them below:\\n\\nMulti-Query Attention (Shazeer, 2019).                    Notably, the 7B model uses multi-head attention\\nwhile the 2B checkpoints use multi-query atten-\\ntion (with   ùëõùë¢ùëö_ùëòùë£_‚Ñéùëíùëéùëëùë† = 1), based on ablations\\nthat showed that multi-query attention works well\\nat small scales (Shazeer, 2019).\\nRoPE Embeddings (Su et al., 2021). Rather than\\nusing absolute positional embeddings, we use ro-\\ntary positional embeddings in each layer; we also\\nshare embeddings across our inputs and outputs\\nto reduce model size.\\nGeGLU Activations (Shazeer, 2020). The stan-\\ndard ReLU non-linearity is replaced by the approx-\", \"answer\": \"The 2B and 7B models are transformer decoder-based models with several improvements proposed after the original transformer paper. The 7B model uses multi-head attention, while the 2B checkpoints use multi-query attention. The 7B model also uses rotary positional embeddings and shares embeddings across its inputs and outputs to reduce model size, while the 2B model does not. Finally, the 7B model uses GeGLU activations, while the 2B model uses standard ReLU non-linearity.\"}}\n",
      "Detected class:  ResearchPaperAnswer\n"
     ]
    }
   ],
   "source": [
    "response = model.generate_content(makePrompt(context)).text\n",
    "\n",
    "print(response)\n",
    "\n",
    "detect_and_print_class_name(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "matr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
